#!/usr/bin/env python3
"""
ë°œí‘œìš© ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
DQN, PPO ì‹¤í—˜ ê²°ê³¼ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ë°œí‘œ ìë£Œ ìƒì„±
"""

import argparse
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
from typing import Dict, List, Any
from datetime import datetime

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'SimHei', 'Malgun Gothic']
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PresentationAnalyzer:
    """ë°œí‘œìš© ê²°ê³¼ ë¶„ì„ê¸°"""
    
    def __init__(self, output_dir: str = "docs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
        self.analysis_results = {
            'summary': {},
            'detailed_comparison': {},
            'performance_metrics': {},
            'recommendations': []
        }
        
        # ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •
        sns.set_style("whitegrid")
        plt.style.use('seaborn-v0_8')
    
    def load_experiment_results(self, results_dirs: Dict[str, str]) -> Dict[str, List[Dict]]:
        """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        all_results = {}
        
        for algorithm, results_dir in results_dirs.items():
            results_path = Path(results_dir)
            algorithm_results = []
            
            if results_path.exists():
                # JSON íŒŒì¼ë“¤ ìˆ˜ì§‘
                for json_file in results_path.glob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            data['source_file'] = str(json_file)
                            algorithm_results.append(data)
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {json_file}: {e}")
                
                logger.info(f"{algorithm}: {len(algorithm_results)}ê°œ ê²°ê³¼ ë¡œë“œ")
            else:
                logger.warning(f"{algorithm} ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì—†ìŒ: {results_path}")
            
            all_results[algorithm] = algorithm_results
        
        return all_results
    
    def analyze_success_rates(self, results: Dict[str, List[Dict]]) -> Dict:
        """ì„±ê³µë¥  ë¶„ì„"""
        success_analysis = {}
        
        for algorithm, data_list in results.items():
            if not data_list:
                continue
                
            successes = []
            
            for data in data_list:
                if 'success' in data:
                    successes.append(data['success'])
                elif 'summary' in data and 'success_rate' in data['summary']:
                    # ë°°ì¹˜ ê²°ê³¼ì¸ ê²½ìš°
                    successes.append(data['summary']['success_rate'])
                elif 'individual_results' in data:
                    # ê°œë³„ ê²°ê³¼ë“¤ ì¶”ì¶œ
                    for result in data['individual_results']:
                        successes.append(result.get('success', False))
            
            if successes:
                success_analysis[algorithm] = {
                    'total_trials': len(successes),
                    'successful_trials': sum(successes),
                    'success_rate': np.mean(successes),
                    'std_deviation': np.std(successes),
                    'success_list': successes
                }
        
        return success_analysis
    
    def analyze_performance_metrics(self, results: Dict[str, List[Dict]]) -> Dict:
        """ì„±ëŠ¥ ì§€í‘œ ë¶„ì„"""
        performance_analysis = {}
        
        for algorithm, data_list in results.items():
            if not data_list:
                continue
            
            metrics = {
                'execution_times': [],
                'solution_lengths': [],
                'vram_usage': [],
                'gpu_utilization': [],
                'training_times': []
            }
            
            for data in data_list:
                # ë‹¨ì¼ ê²°ê³¼
                if 'execution_time' in data:
                    metrics['execution_times'].append(data['execution_time'])
                
                if 'solution_length' in data:
                    metrics['solution_lengths'].append(data['solution_length'])
                
                if 'performance' in data:
                    perf = data['performance']
                    metrics['vram_usage'].append(perf.get('vram_usage', 0))
                    metrics['gpu_utilization'].append(perf.get('gpu_utilization', 0))
                
                # ë°°ì¹˜ ê²°ê³¼
                if 'individual_results' in data:
                    for result in data['individual_results']:
                        metrics['execution_times'].append(result.get('execution_time', 0))
                        metrics['solution_lengths'].append(result.get('solution_length', 0))
                        metrics['vram_usage'].append(result.get('vram_usage', 0))
            
            # í†µê³„ ê³„ì‚°
            performance_analysis[algorithm] = {}
            for metric_name, values in metrics.items():
                if values:
                    performance_analysis[algorithm][metric_name] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                        'median': np.median(values),
                        'values': values
                    }
        
        return performance_analysis
    
    def create_success_rate_comparison(self, success_analysis: Dict) -> str:
        """ì„±ê³µë¥  ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        algorithms = list(success_analysis.keys())
        success_rates = [success_analysis[alg]['success_rate'] for alg in algorithms]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
        
        # ì„±ê³µë¥  ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax1.bar(algorithms, success_rates, color=colors[:len(algorithms)], alpha=0.8)
        ax1.set_title('ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ê³µë¥  ë¹„êµ', fontsize=14, fontweight='bold')
        ax1.set_ylabel('ì„±ê³µë¥ ')
        ax1.set_ylim(0, 1)
        
        # ê°’ í‘œì‹œ
        for bar, rate in zip(bars, success_rates):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{rate:.2%}', ha='center', va='bottom', fontweight='bold')
        
        # ì„±ê³µë¥  ë¶„í¬ ë°•ìŠ¤í”Œë¡¯
        success_distributions = []
        labels = []
        
        for alg in algorithms:
            if success_analysis[alg]['success_list']:
                success_distributions.append(success_analysis[alg]['success_list'])
                labels.append(alg)
        
        if success_distributions:
            ax2.boxplot(success_distributions, labels=labels)
            ax2.set_title('ì„±ê³µë¥  ë¶„í¬', fontsize=14, fontweight='bold')
            ax2.set_ylabel('ì„±ê³µë¥ ')
        
        plt.tight_layout()
        save_path = self.output_dir / "success_rate_comparison.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ì„±ê³µë¥  ë¹„êµ ì°¨íŠ¸ ì €ì¥: {save_path}")
        return str(save_path)
    
    def create_performance_comparison(self, performance_analysis: Dict) -> str:
        """ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        algorithms = list(performance_analysis.keys())
        
        if len(algorithms) < 2:
            logger.warning("ë¹„êµí•  ì•Œê³ ë¦¬ì¦˜ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return ""
        
        # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
        metrics_to_plot = ['execution_times', 'solution_lengths', 'vram_usage']
        metric_labels = ['ì‹¤í–‰ ì‹œê°„ (ì´ˆ)', 'í•´ê²° ê²½ë¡œ ê¸¸ì´', 'VRAM ì‚¬ìš©ëŸ‰ (MB)']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):
            ax = axes[i]
            
            means = []
            stds = []
            alg_names = []
            
            for alg in algorithms:
                if metric in performance_analysis[alg]:
                    means.append(performance_analysis[alg][metric]['mean'])
                    stds.append(performance_analysis[alg][metric]['std'])
                    alg_names.append(alg)
            
            if means:
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(alg_names)]
                bars = ax.bar(alg_names, means, yerr=stds, capsize=5, 
                             color=colors, alpha=0.8)
                
                ax.set_title(f'{label} ë¹„êµ', fontsize=12, fontweight='bold')
                ax.set_ylabel(label)
                
                # ê°’ í‘œì‹œ
                for bar, mean, std in zip(bars, means, stds):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                           f'{mean:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        save_path = self.output_dir / "performance_comparison.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {save_path}")
        return str(save_path)
    
    def create_learning_curves_comparison(self, results: Dict[str, List[Dict]]) -> str:
        """í•™ìŠµ ê³¡ì„  ë¹„êµ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        colors = {'DQN': '#FF6B6B', 'PPO': '#4ECDC4', 'ACO': '#45B7D1'}
        
        for algorithm, data_list in results.items():
            color = colors.get(algorithm, '#666666')
            
            for data in data_list:
                learning_curves = data.get('learning_curves', {})
                
                # ì—í”¼ì†Œë“œ ë³´ìƒ
                if 'rewards' in learning_curves:
                    episodes = learning_curves.get('episodes', range(len(learning_curves['rewards'])))
                    rewards = learning_curves['rewards']
                    
                    # ì´ë™ í‰ê·  ê³„ì‚°
                    window = min(50, len(rewards) // 10)
                    if window > 1:
                        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
                        ax1.plot(episodes[window-1:], moving_avg, 
                               label=f'{algorithm} (ì´ë™í‰ê· )', color=color, linewidth=2)
                    else:
                        ax1.plot(episodes, rewards, label=algorithm, color=color, alpha=0.7)
                
                # ì„±ê³µë¥ 
                if 'success_rates' in learning_curves:
                    episodes = range(len(learning_curves['success_rates']))
                    ax2.plot(episodes, learning_curves['success_rates'], 
                           label=algorithm, color=color, linewidth=2)
                
                # ì •ì±… ì†ì‹¤ (PPOë§Œ)
                if algorithm == 'PPO' and 'policy_losses' in learning_curves:
                    updates = range(len(learning_curves['policy_losses']))
                    ax3.plot(updates, learning_curves['policy_losses'], 
                           label='ì •ì±… ì†ì‹¤', color=color, linewidth=2)
                
                # ê°€ì¹˜ ì†ì‹¤
                if 'value_losses' in learning_curves:
                    updates = range(len(learning_curves['value_losses']))
                    ax4.plot(updates, learning_curves['value_losses'], 
                           label=f'{algorithm} ê°€ì¹˜ ì†ì‹¤', color=color, linewidth=2)
        
        # ì¶• ì„¤ì •
        ax1.set_title('ì—í”¼ì†Œë“œ ë³´ìƒ ë³€í™”', fontsize=12, fontweight='bold')
        ax1.set_xlabel('ì—í”¼ì†Œë“œ')
        ax1.set_ylabel('ë³´ìƒ')
        ax1.legend()
        ax1.grid(True)
        
        ax2.set_title('ì„±ê³µë¥  ë³€í™”', fontsize=12, fontweight='bold')
        ax2.set_xlabel('ì—í”¼ì†Œë“œ')
        ax2.set_ylabel('ì„±ê³µë¥ ')
        ax2.set_ylim(0, 1)
        ax2.legend()
        ax2.grid(True)
        
        ax3.set_title('ì •ì±… ì†ì‹¤ ë³€í™” (PPO)', fontsize=12, fontweight='bold')
        ax3.set_xlabel('ì—…ë°ì´íŠ¸')
        ax3.set_ylabel('ì •ì±… ì†ì‹¤')
        ax3.legend()
        ax3.grid(True)
        
        ax4.set_title('ê°€ì¹˜ ì†ì‹¤ ë³€í™”', fontsize=12, fontweight='bold')
        ax4.set_xlabel('ì—…ë°ì´íŠ¸')
        ax4.set_ylabel('ê°€ì¹˜ ì†ì‹¤')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        save_path = self.output_dir / "learning_curves_comparison.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"í•™ìŠµ ê³¡ì„  ë¹„êµ ì €ì¥: {save_path}")
        return str(save_path)
    
    def generate_presentation_summary(self, success_analysis: Dict, 
                                    performance_analysis: Dict) -> str:
        """ë°œí‘œìš© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        
        summary_content = f"""# ğŸ¯ ê°•í™”í•™ìŠµ ë¯¸ë¡œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì—°êµ¬

> **ë°œí‘œì**: ê¸°ê³„ê³µí•™ê³¼ 3í•™ë…„  
> **ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}  
> **ì£¼ì œ**: DQN vs PPO ë¯¸ë¡œ í•´ê²° ì„±ëŠ¥ ë¹„êµ

---

## ğŸ“Š í•µì‹¬ ê²°ê³¼ ìš”ì•½

### ì„±ê³µë¥  ë¹„êµ
"""
        
        # ì„±ê³µë¥  í‘œ
        for algorithm, analysis in success_analysis.items():
            summary_content += f"- **{algorithm}**: {analysis['success_rate']:.1%} ({analysis['successful_trials']}/{analysis['total_trials']})\n"
        
        summary_content += "\n### ì„±ëŠ¥ ì§€í‘œ ë¹„êµ\n\n"
        
        # ì„±ëŠ¥ ì§€í‘œ í‘œ
        summary_content += "| ì•Œê³ ë¦¬ì¦˜ | í‰ê·  ì‹¤í–‰ì‹œê°„ | í‰ê·  ê²½ë¡œê¸¸ì´ | í‰ê·  VRAM |\n"
        summary_content += "|---------|-------------|-------------|----------|\n"
        
        for algorithm, metrics in performance_analysis.items():
            exec_time = metrics.get('execution_times', {}).get('mean', 0)
            path_length = metrics.get('solution_lengths', {}).get('mean', 0)
            vram = metrics.get('vram_usage', {}).get('mean', 0)
            
            summary_content += f"| {algorithm} | {exec_time:.2f}ì´ˆ | {path_length:.1f} | {vram:.0f}MB |\n"
        
        # ê¶Œì¥ì‚¬í•­ ë° ê²°ë¡ 
        summary_content += f"""

---

## ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­

### 1. ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„± ë¹„êµ
- **DQN**: Value-based ê°•í™”í•™ìŠµ, Experience Replay í™œìš©
- **PPO**: Policy-based ê°•í™”í•™ìŠµ, ì•ˆì •ì ì¸ ì •ì±… ê°œì„ 

### 2. ì„±ëŠ¥ ë¶„ì„
"""
        
        # ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ ì°¾ê¸°
        best_success = max(success_analysis.items(), key=lambda x: x[1]['success_rate'])
        summary_content += f"- **ì„±ê³µë¥  ìµœê³ **: {best_success[0]} ({best_success[1]['success_rate']:.1%})\n"
        
        if performance_analysis:
            fastest_alg = min(performance_analysis.items(), 
                            key=lambda x: x[1].get('execution_times', {}).get('mean', float('inf')))
            summary_content += f"- **ì‹¤í–‰ì†ë„ ìµœê³ **: {fastest_alg[0]} ({fastest_alg[1].get('execution_times', {}).get('mean', 0):.2f}ì´ˆ)\n"
        
        summary_content += """

### 3. RTX 3060 ìµœì í™” íš¨ê³¼
- ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ VRAM íš¨ìœ¨ì„± ê°œì„ 
- ë¯¸ë¡œ í¬ê¸°ë³„ ë™ì  ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

---

## ğŸš€ í–¥í›„ ê³„íš

### ë‹¨ê¸° ëª©í‘œ
1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: Grid Search ì ìš©
2. **ë” ë³µì¡í•œ ë¯¸ë¡œ**: ë™ì  ì¥ì• ë¬¼, ë‹¤ì¤‘ ëª©í‘œ
3. **ì•™ìƒë¸” ë°©ë²•**: DQN + PPO í•˜ì´ë¸Œë¦¬ë“œ

### ì¥ê¸° ëª©í‘œ
1. **Isaac Sim í†µí•©**: 3D ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ í™•ì¥
2. **ì‹¤ì œ ë¡œë´‡ ì ìš©**: ë„¤ë¹„ê²Œì´ì…˜ ì‹œìŠ¤í…œ ê°œë°œ
3. **ì‚°ì—… ì‘ìš©**: ì°½ê³  ìë™í™”, AGV ê²½ë¡œ ê³„íš

---

## ğŸ“‹ ë°œí‘œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ìˆ ì  ë‚´ìš©
- [ ] DQN, PPO ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ ì„¤ëª… (3ë¶„)
- [ ] ì‹¤í—˜ ì„¤ì • ë° í™˜ê²½ ì†Œê°œ (2ë¶„)
- [ ] ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ë°œí‘œ (4ë¶„)
- [ ] ê²°ë¡  ë° í–¥í›„ ê³„íš (1ë¶„)

### ì‹œì—° ì¤€ë¹„
- [ ] í•™ìŠµ ê³¼ì • ë™ì˜ìƒ
- [ ] ë¯¸ë¡œ í•´ê²° ì‹œì—°
- [ ] ì„±ëŠ¥ ê·¸ë˜í”„ ì‹¤ì‹œê°„ í‘œì‹œ

### ì§ˆë¬¸ ëŒ€ë¹„
- [ ] ì™œ ê°•í™”í•™ìŠµì„ ì„ íƒí–ˆëŠ”ê°€?
- [ ] DQNê³¼ PPOì˜ ì°¨ì´ì ì€?
- [ ] ì‹¤ì œ ë¡œë´‡ì— ì–´ë–»ê²Œ ì ìš©í•  ê²ƒì¸ê°€?
- [ ] Isaac Sim í™•ì¥ ê³„íšì˜ êµ¬ì²´ì  ë‚´ìš©?

---

**ğŸ‰ ì„±ê³µì ì¸ ë°œí‘œë¥¼ ìœ„í•´ í™”ì´íŒ…!**
"""
        
        # íŒŒì¼ ì €ì¥
        summary_path = self.output_dir / "presentation_summary.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        logger.info(f"ë°œí‘œ ìš”ì•½ ì €ì¥: {summary_path}")
        return str(summary_path)
    
    def create_comparison_table(self, success_analysis: Dict, 
                              performance_analysis: Dict) -> str:
        """ìƒì„¸ ë¹„êµ í‘œ ìƒì„±"""
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        comparison_data = []
        
        for algorithm in success_analysis.keys():
            row = {'ì•Œê³ ë¦¬ì¦˜': algorithm}
            
            # ì„±ê³µë¥  ë°ì´í„°
            success_data = success_analysis[algorithm]
            row['ì„±ê³µë¥ '] = f"{success_data['success_rate']:.1%}"
            row['ì„±ê³µ_ì‹œí–‰'] = f"{success_data['successful_trials']}/{success_data['total_trials']}"
            
            # ì„±ëŠ¥ ë°ì´í„°
            if algorithm in performance_analysis:
                perf_data = performance_analysis[algorithm]
                
                if 'execution_times' in perf_data:
                    row['í‰ê· _ì‹¤í–‰ì‹œê°„'] = f"{perf_data['execution_times']['mean']:.2f}ì´ˆ"
                
                if 'solution_lengths' in perf_data:
                    row['í‰ê· _ê²½ë¡œê¸¸ì´'] = f"{perf_data['solution_lengths']['mean']:.1f}"
                
                if 'vram_usage' in perf_data:
                    row['í‰ê· _VRAM'] = f"{perf_data['vram_usage']['mean']:.0f}MB"
                
                if 'gpu_utilization' in perf_data:
                    row['í‰ê· _GPUì‚¬ìš©ë¥ '] = f"{perf_data['gpu_utilization']['mean']:.1f}%"
            
            comparison_data.append(row)
        
        # CSV ì €ì¥
        df = pd.DataFrame(comparison_data)
        csv_path = self.output_dir / "detailed_comparison.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"ìƒì„¸ ë¹„êµí‘œ ì €ì¥: {csv_path}")
        return str(csv_path)
    
    def run_complete_analysis(self, results_dirs: Dict[str, str]) -> Dict[str, str]:
        """ì™„ì „ ë¶„ì„ ì‹¤í–‰"""
        logger.info("ë°œí‘œìš© ê²°ê³¼ ë¶„ì„ ì‹œì‘...")
        
        # ê²°ê³¼ ë¡œë“œ
        results = self.load_experiment_results(results_dirs)
        
        if not any(results.values()):
            logger.error("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ë¶„ì„ ì‹¤í–‰
        success_analysis = self.analyze_success_rates(results)
        performance_analysis = self.analyze_performance_metrics(results)
        
        # ì‹œê°í™” ìƒì„±
        output_files = {}
        
        if success_analysis:
            output_files['success_chart'] = self.create_success_rate_comparison(success_analysis)
        
        if performance_analysis:
            output_files['performance_chart'] = self.create_performance_comparison(performance_analysis)
        
        output_files['learning_curves'] = self.create_learning_curves_comparison(results)
        output_files['summary'] = self.generate_presentation_summary(success_analysis, performance_analysis)
        output_files['comparison_table'] = self.create_comparison_table(success_analysis, performance_analysis)
        
        # ì¢…í•© ê²°ê³¼ ì €ì¥
        analysis_summary = {
            'analysis_date': datetime.now().isoformat(),
            'algorithms_analyzed': list(results.keys()),
            'success_analysis': success_analysis,
            'performance_analysis': performance_analysis,
            'output_files': output_files
        }
        
        summary_path = self.output_dir / "complete_analysis.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_summary, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ì¢…í•© ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥: {self.output_dir}")
        return output_files

def main():
    parser = argparse.ArgumentParser(description='ë°œí‘œìš© ê²°ê³¼ ë¶„ì„')
    
    parser.add_argument('--dqn-results', type=str, default='results/dqn',
                       help='DQN ê²°ê³¼ ë””ë ‰í„°ë¦¬')
    parser.add_argument('--ppo-results', type=str, default='results/ppo',
                       help='PPO ê²°ê³¼ ë””ë ‰í„°ë¦¬')
    parser.add_argument('--aco-results', type=str, default='results/aco',
                       help='ACO ê²°ê³¼ ë””ë ‰í„°ë¦¬ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--output', type=str, default='docs',
                       help='ì¶œë ¥ ë””ë ‰í„°ë¦¬')
    
    args = parser.parse_args()
    
    # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì„¤ì •
    results_dirs = {}
    
    if Path(args.dqn_results).exists():
        results_dirs['DQN'] = args.dqn_results
    
    if Path(args.ppo_results).exists():
        results_dirs['PPO'] = args.ppo_results
    
    if args.aco_results and Path(args.aco_results).exists():
        results_dirs['ACO'] = args.aco_results
    
    if not results_dirs:
        print("âŒ ë¶„ì„í•  ê²°ê³¼ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹¤í—˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
        print("  python scripts/train_dqn.py --maze-id 000001 --output results/dqn/test.json")
        print("  python scripts/train_ppo.py --maze-id 000001 --output results/ppo/test.json")
        return
    
    print(f"ğŸ“Š ë¶„ì„ ì‹œì‘: {list(results_dirs.keys())}")
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = PresentationAnalyzer(args.output)
    output_files = analyzer.run_complete_analysis(results_dirs)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í„°ë¦¬: {args.output}")
    print("\nğŸ“Š ìƒì„±ëœ íŒŒì¼ë“¤:")
    
    for file_type, file_path in output_files.items():
        if file_path:
            print(f"  ğŸ“„ {file_type}: {file_path}")
    
    print(f"\nğŸ¯ ë°œí‘œ ì¤€ë¹„:")
    print(f"  1. ìš”ì•½ ë¬¸ì„œ: {output_files.get('summary', 'N/A')}")
    print(f"  2. ì„±ê³µë¥  ì°¨íŠ¸: {output_files.get('success_chart', 'N/A')}")
    print(f"  3. ì„±ëŠ¥ ë¹„êµ: {output_files.get('performance_chart', 'N/A')}")
    print(f"  4. í•™ìŠµ ê³¡ì„ : {output_files.get('learning_curves', 'N/A')}")

if __name__ == "__main__":
    main()