#!/usr/bin/env python3
"""
ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
ACO, ACO+CNN, ACO+DeepForest, DQN í†µí•© ë²¤ì¹˜ë§ˆí¬
"""

import argparse
import json
import time
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import logging
import sys
from typing import List, Dict
import algorithms.ppo_benchmark_wrapper as PPOBenchmarkWrapper

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from algorithms.dqn_benchmark_wrapper import DQNBenchmarkWrapper
from utils.maze_io import get_loader
from utils.profiler import get_profiler
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AlgorithmComparator:
    """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "results/comparison"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¹„êµ ê²°ê³¼ ì €ì¥ìš©
        self.results = []
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ë˜í¼ ì´ˆê¸°í™”
        self.algorithm_wrappers = {
            'DQN': DQNBenchmarkWrapper(
                training_episodes=1000,  # ë¹„êµìš© ë‹¨ì¶•
                max_steps=1000,
                model_save_dir="models/comparison"
            ),
            'PPO': PPOBenchmarkWrapper(
                total_timesteps=100000,  # ë¹„êµìš© ë‹¨ì¶•
                max_episode_steps=1000,
                model_save_dir="models/comparison"
            )
        }
        
    def run_single_comparison(self, 
                            algorithms: List[str], 
                            maze_id: str, 
                            subset: str = "test") -> Dict:
        """ë‹¨ì¼ ë¯¸ë¡œì— ëŒ€í•œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ"""
        
        logger.info(f"ë¯¸ë¡œ {maze_id} ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹œì‘: {algorithms}")
        
        maze_results = {
            'maze_id': maze_id,
            'algorithms': {}
        }
        
        for algo in algorithms:
            logger.info(f"{algo} ì‹¤í–‰ ì¤‘...")
            
            try:
                if algo == 'DQN':
                    # DQN ì‹¤í–‰
                    result = self.algorithm_wrappers['DQN'].run_benchmark(
                        maze_id=maze_id,
                        subset=subset
                    )
                    
                    algo_result = {
                        'algorithm': result.algorithm,
                        'success': result.solution_found,
                        'execution_time': result.execution_time,
                        'solution_length': result.solution_length,
                        'total_steps': result.total_steps,
                        'vram_usage': result.vram_usage,
                        'gpu_utilization': result.gpu_utilization,
                        'cpu_utilization': result.cpu_utilization,
                        'power_consumption': result.power_consumption,
                        'failure_reason': result.failure_reason
                    }
                    
                elif algo == 'PPO':
                    # PPO ì‹¤í–‰
                    result = self.algorithm_wrappers['PPO'].run_benchmark(
                        maze_id=maze_id,
                        subset=subset
                    )
                    
                    algo_result = {
                        'algorithm': result.algorithm,
                        'success': result.solution_found,
                        'execution_time': result.execution_time,
                        'solution_length': result.solution_length,
                        'total_steps': result.total_steps,
                        'vram_usage': result.vram_usage,
                        'gpu_utilization': result.gpu_utilization,
                        'cpu_utilization': result.cpu_utilization,
                        'power_consumption': result.power_consumption,
                        'failure_reason': result.failure_reason,
                        'training_stats': result.additional_metrics.get('training_stats', {})
                    }
                    
                elif algo in ['ACO', 'ACO_CNN', 'ACO_DeepForest']:
                    # ê¸°ì¡´ ACO ì•Œê³ ë¦¬ì¦˜ë“¤ ì‹¤í–‰ (êµ¬í˜„ëœ ê²ƒìœ¼ë¡œ ê°€ì •)
                    algo_result = self._run_aco_algorithm(algo, maze_id, subset)
                    
                else:
                    logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜: {algo}")
                    continue
                    
                maze_results['algorithms'][algo] = algo_result
                logger.info(f"{algo} ì™„ë£Œ: {'ì„±ê³µ' if algo_result['success'] else 'ì‹¤íŒ¨'}")
                
            except Exception as e:
                logger.error(f"{algo} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                maze_results['algorithms'][algo] = {
                    'algorithm': algo,
                    'success': False,
                    'error': str(e)
                }
        
        return maze_results
    
    def _run_aco_algorithm(self, algo: str, maze_id: str, subset: str) -> Dict:
        """ACO ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ (ê¸°ì¡´ êµ¬í˜„ ì—°ë™)"""
        # ê¸°ì¡´ ACO êµ¬í˜„ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ëª¨ì˜ ê²°ê³¼ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” ê¸°ì¡´ êµ¬í˜„ëœ ACO í´ë˜ìŠ¤ë“¤ì„ í˜¸ì¶œí•´ì•¼ í•¨
        
        logger.warning(f"{algo} ëª¨ì˜ ì‹¤í–‰ - ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ êµì²´ í•„ìš”")
        
        # ëª¨ì˜ ê²°ê³¼ (ì‹¤ì œ êµ¬í˜„ ì‹œ ì œê±°)
        import random
        success = random.choice([True, False])
        
        return {
            'algorithm': algo,
            'success': success,
            'execution_time': random.uniform(1.0, 10.0),
            'solution_length': random.randint(20, 100) if success else 0,
            'total_steps': random.randint(50, 500),
            'vram_usage': random.uniform(100, 1000),
            'gpu_utilization': random.uniform(10, 80),
            'cpu_utilization': random.uniform(20, 90),
            'power_consumption': random.uniform(50, 150),
            'failure_reason': None if success else 'timeout'
        }
    
    def run_batch_comparison(self, 
                           algorithms: List[str], 
                           maze_ids: List[str], 
                           subset: str = "test") -> List[Dict]:
        """ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ"""
        
        logger.info(f"ë°°ì¹˜ ë¹„êµ ì‹œì‘: {len(maze_ids)}ê°œ ë¯¸ë¡œ, {len(algorithms)}ê°œ ì•Œê³ ë¦¬ì¦˜")
        
        all_results = []
        
        for i, maze_id in enumerate(maze_ids):
            logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(maze_ids)} - ë¯¸ë¡œ {maze_id}")
            
            try:
                maze_result = self.run_single_comparison(algorithms, maze_id, subset)
                all_results.append(maze_result)
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                if (i + 1) % 5 == 0:
                    self._save_intermediate_results(all_results, f"batch_comparison_partial_{i+1}.json")
                    
            except Exception as e:
                logger.error(f"ë¯¸ë¡œ {maze_id} ë¹„êµ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info("ë°°ì¹˜ ë¹„êµ ì™„ë£Œ")
        return all_results
    
    def _save_intermediate_results(self, results: List[Dict], filename: str):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {filepath}")
    
    def analyze_results(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ë¶„ì„ ë° í†µê³„ ìƒì„±"""
        
        logger.info("ê²°ê³¼ ë¶„ì„ ì‹œì‘...")
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ì§‘ê³„
        algo_stats = {}
        
        all_algorithms = set()
        for result in results:
            all_algorithms.update(result['algorithms'].keys())
        
        for algo in all_algorithms:
            algo_results = []
            for result in results:
                if algo in result['algorithms']:
                    algo_results.append(result['algorithms'][algo])
            
            if algo_results:
                successful = [r for r in algo_results if r.get('success', False)]
                
                algo_stats[algo] = {
                    'total_mazes': len(algo_results),
                    'successful_mazes': len(successful),
                    'success_rate': len(successful) / len(algo_results),
                    'avg_execution_time': np.mean([r.get('execution_time', 0) for r in algo_results]),
                    'avg_solution_length': np.mean([r.get('solution_length', 0) for r in successful]) if successful else 0,
                    'avg_vram_usage': np.mean([r.get('vram_usage', 0) for r in algo_results]),
                    'avg_gpu_utilization': np.mean([r.get('gpu_utilization', 0) for r in algo_results]),
                    'avg_power_consumption': np.mean([r.get('power_consumption', 0) for r in algo_results])
                }
        
        analysis = {
            'summary': {
                'total_mazes': len(results),
                'algorithms_tested': list(all_algorithms),
                'analysis_date': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'algorithm_stats': algo_stats,
            'detailed_results': results
        }
        
        return analysis
    
    def generate_report(self, analysis: Dict, save_plots: bool = True) -> str:
        """ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        
        logger.info("ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # JSON ê²°ê³¼ ì €ì¥
        analysis_file = self.output_dir / "algorithm_comparison_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # CSV ìš”ì•½ ì €ì¥
        algo_stats = analysis['algorithm_stats']
        df = pd.DataFrame.from_dict(algo_stats, orient='index')
        csv_file = self.output_dir / "algorithm_comparison_summary.csv"
        df.to_csv(csv_file, encoding='utf-8-sig')
        
        # ì‹œê°í™” ìƒì„±
        if save_plots:
            self._create_comparison_plots(algo_stats)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
        report_file = self.output_dir / "algorithm_comparison_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ğŸŒ€ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"ë¶„ì„ ì¼ì‹œ: {analysis['summary']['analysis_date']}\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë¯¸ë¡œ ìˆ˜: {analysis['summary']['total_mazes']}\n")
            f.write(f"ë¹„êµ ì•Œê³ ë¦¬ì¦˜: {', '.join(analysis['summary']['algorithms_tested'])}\n\n")
            
            f.write("ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n")
            f.write("-" * 30 + "\n")
            
            for algo, stats in algo_stats.items():
                f.write(f"\nğŸ”¸ {algo}\n")
                f.write(f"  ì„±ê³µë¥ : {stats['success_rate']:.2%}\n")
                f.write(f"  í‰ê·  ì‹¤í–‰ì‹œê°„: {stats['avg_execution_time']:.2f}ì´ˆ\n")
                f.write(f"  í‰ê·  ê²½ë¡œê¸¸ì´: {stats['avg_solution_length']:.1f}\n")
                f.write(f"  í‰ê·  VRAM: {stats['avg_vram_usage']:.1f}MB\n")
                f.write(f"  í‰ê·  GPU ì‚¬ìš©ë¥ : {stats['avg_gpu_utilization']:.1f}%\n")
                f.write(f"  í‰ê·  ì „ë ¥ì†Œë¹„: {stats['avg_power_consumption']:.1f}W\n")
            
            # ìˆœìœ„ ë§¤ê¸°ê¸°
            f.write(f"\nğŸ† ì„±ëŠ¥ ìˆœìœ„\n")
            f.write("-" * 30 + "\n")
            
            # ì„±ê³µë¥  ê¸°ì¤€ ìˆœìœ„
            success_ranking = sorted(algo_stats.items(), 
                                   key=lambda x: x[1]['success_rate'], 
                                   reverse=True)
            
            f.write("ì„±ê³µë¥  ìˆœìœ„:\n")
            for i, (algo, stats) in enumerate(success_ranking, 1):
                f.write(f"  {i}. {algo}: {stats['success_rate']:.2%}\n")
            
            # ì‹¤í–‰ì‹œê°„ ê¸°ì¤€ ìˆœìœ„ (ì„±ê³µí•œ ì¼€ì´ìŠ¤ë§Œ)
            time_ranking = sorted([(algo, stats) for algo, stats in algo_stats.items() 
                                 if stats['success_rate'] > 0], 
                                key=lambda x: x[1]['avg_execution_time'])
            
            f.write("\nì‹¤í–‰ì†ë„ ìˆœìœ„ (ë¹ ë¥¸ ìˆœ):\n")
            for i, (algo, stats) in enumerate(time_ranking, 1):
                f.write(f"  {i}. {algo}: {stats['avg_execution_time']:.2f}ì´ˆ\n")
        
        logger.info(f"ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
        return str(report_file)
    
    def _create_comparison_plots(self, algo_stats: Dict):
        """ë¹„êµ ì‹œê°í™” ìƒì„±"""
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í”Œë¡¯
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        algorithms = list(algo_stats.keys())
        
        # ì„±ê³µë¥ 
        success_rates = [algo_stats[algo]['success_rate'] for algo in algorithms]
        ax1.bar(algorithms, success_rates, color='skyblue', alpha=0.7)
        ax1.set_title('ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ê³µë¥ ')
        ax1.set_ylabel('ì„±ê³µë¥ ')
        ax1.set_ylim(0, 1)
        
        # ì‹¤í–‰ì‹œê°„
        exec_times = [algo_stats[algo]['avg_execution_time'] for algo in algorithms]
        ax2.bar(algorithms, exec_times, color='lightcoral', alpha=0.7)
        ax2.set_title('ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  ì‹¤í–‰ì‹œê°„')
        ax2.set_ylabel('ì‹¤í–‰ì‹œê°„ (ì´ˆ)')
        
        # VRAM ì‚¬ìš©ëŸ‰
        vram_usage = [algo_stats[algo]['avg_vram_usage'] for algo in algorithms]
        ax3.bar(algorithms, vram_usage, color='lightgreen', alpha=0.7)
        ax3.set_title('ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  VRAM ì‚¬ìš©ëŸ‰')
        ax3.set_ylabel('VRAM (MB)')
        
        # ì „ë ¥ ì†Œë¹„
        power_consumption = [algo_stats[algo]['avg_power_consumption'] for algo in algorithms]
        ax4.bar(algorithms, power_consumption, color='gold', alpha=0.7)
        ax4.set_title('ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  ì „ë ¥ ì†Œë¹„')
        ax4.set_ylabel('ì „ë ¥ (W)')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "algorithm_comparison_plots.png", 
                   dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info("ë¹„êµ í”Œë¡¯ ì €ì¥ ì™„ë£Œ")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if 'DQN' in self.algorithm_wrappers:
            self.algorithm_wrappers['DQN'].cleanup()
        logger.info("ì•Œê³ ë¦¬ì¦˜ ë¹„êµê¸° ì •ë¦¬ ì™„ë£Œ")

def main():
    parser = argparse.ArgumentParser(description='ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ')
    
    parser.add_argument('--algorithms', type=str, required=True,
                       help='ë¹„êµí•  ì•Œê³ ë¦¬ì¦˜ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: ACO,DQN,ACO_CNN)')
    parser.add_argument('--maze-ids', type=str, required=True,
                       help='í…ŒìŠ¤íŠ¸í•  ë¯¸ë¡œ IDë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 000001,000002,000003)')
    parser.add_argument('--subset', type=str, default='test',
                       choices=['train', 'valid', 'test'], help='ë°ì´í„°ì…‹ ë¶„í• ')
    parser.add_argument('--output-dir', type=str, default='results/comparison',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬')
    parser.add_argument('--no-plots', action='store_true', help='í”Œë¡¯ ìƒì„± ì•ˆ í•¨')
    
    args = parser.parse_args()
    
    # ì¸ìˆ˜ íŒŒì‹±
    algorithms = [algo.strip() for algo in args.algorithms.split(',')]
    maze_ids = [maze_id.strip() for maze_id in args.maze_ids.split(',')]
    
    logger.info(f"ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹œì‘:")
    logger.info(f"  ì•Œê³ ë¦¬ì¦˜: {algorithms}")
    logger.info(f"  ë¯¸ë¡œ ID: {maze_ids}")
    logger.info(f"  ë°ì´í„°ì…‹: {args.subset}")
    
    # ë¹„êµ ì‹¤í–‰
    comparator = AlgorithmComparator(args.output_dir)
    
    try:
        # ë°°ì¹˜ ë¹„êµ ì‹¤í–‰
        results = comparator.run_batch_comparison(algorithms, maze_ids, args.subset)
        
        # ê²°ê³¼ ë¶„ì„
        analysis = comparator.analyze_results(results)
        
        # ë³´ê³ ì„œ ìƒì„±
        report_file = comparator.generate_report(analysis, not args.no_plots)
        
        print(f"\nğŸ‰ ë¹„êµ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í„°ë¦¬: {args.output_dir}")
        print(f"ğŸ“Š ë³´ê³ ì„œ: {report_file}")
        print(f"ğŸ“ˆ ìš”ì•½ CSV: {args.output_dir}/algorithm_comparison_summary.csv")
        
        if not args.no_plots:
            print(f"ğŸ“Š ì‹œê°í™”: {args.output_dir}/algorithm_comparison_plots.png")
        
    except Exception as e:
        logger.error(f"ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise
    finally:
        comparator.cleanup()

if __name__ == "__main__":
    main()