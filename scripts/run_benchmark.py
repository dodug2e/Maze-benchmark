#!/usr/bin/env python3
"""
ë¯¸ë¡œ ë²¤ì¹˜ë§ˆí¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
CNN, Deep Forest, í•˜ì´ë¸Œë¦¬ë“œ ACO í†µí•© ì‹¤í–‰
"""

import argparse
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë“ˆ import
from utils.maze_io import get_loader
from utils.profiler import get_profiler, profile_execution
from scripts.train_ml_models import MLModelTrainer
from algorithms.hybrid_aco import HybridSolver

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('benchmark.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class MazeBenchmark:
    """ë¯¸ë¡œ ì•Œê³ ë¦¬ì¦˜ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.maze_loader = get_loader(self.config.get('dataset_path', 'datasets'))
        self.profiler = get_profiler()
        
        # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì„¤ì •
        self.output_dir = Path(self.config.get('output_dir', 'output'))
        self.output_dir.mkdir(exist_ok=True)
        
        self.models_dir = self.output_dir / 'models'
        self.results_dir = self.output_dir / 'results'
        self.logs_dir = self.output_dir / 'logs'
        
        for directory in [self.models_dir, self.results_dir, self.logs_dir]:
            directory.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥
        self.benchmark_results = {}
        
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {config_path}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            logger.error(f"ì„¤ì • íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    def check_system_requirements(self) -> bool:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        logger.info("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
        
        # GPU ìƒíƒœ í™•ì¸
        gpu_status = self.profiler.check_rtx3060_limits()
        logger.info(f"ì´ˆê¸° VRAM ì‚¬ìš©ë¥ : {gpu_status['vram_utilization_percent']:.1f}%")
        
        hardware_limits = self.config.get('hardware', {})
        gpu_limit = hardware_limits.get('gpu_memory_limit_mb', 6144)
        
        if gpu_status['current_metrics']['vram_used_mb'] > gpu_limit * 0.8:
            logger.warning(f"VRAM ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤: {gpu_status['current_metrics']['vram_used_mb']:.1f}MB")
        
        # GPU ê²½ê³  ì¶œë ¥
        if gpu_status['warnings']:
            logger.warning("GPU ìƒíƒœ ê²½ê³ :")
            for warning in gpu_status['warnings']:
                logger.warning(f"  - {warning}")
        
        # ë°ì´í„°ì…‹ í™•ì¸
        try:
            train_samples = len(self.maze_loader.get_sample_ids('train'))
            valid_samples = len(self.maze_loader.get_sample_ids('valid'))
            test_samples = len(self.maze_loader.get_sample_ids('test'))
            
            logger.info(f"ë°ì´í„°ì…‹ í¬ê¸°: train={train_samples}, valid={valid_samples}, test={test_samples}")
            
            if train_samples == 0:
                logger.error("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return False
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def train_ml_models(self) -> Dict:
        """ML ëª¨ë¸ë“¤ í•™ìŠµ"""
        logger.info("\n" + "="*60)
        logger.info("ML ëª¨ë¸ í•™ìŠµ ë‹¨ê³„")
        logger.info("="*60)
        
        # ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        cnn_model_path = self.models_dir / 'best_cnn_model.pth'
        df_model_path = self.models_dir / 'deep_forest_model.joblib'
        
        force_retrain = self.config.get('force_retrain', False)
        
        if (cnn_model_path.exists() and df_model_path.exists() and not force_retrain):
            logger.info("ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë°œê²¬, í•™ìŠµ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            logger.info("ì¬í•™ìŠµì„ ì›í•˜ë©´ 'force_retrain': true ì„¤ì •í•˜ì„¸ìš”.")
            
            return {
                'cnn': {'model_path': str(cnn_model_path), 'skipped': True},
                'deep_forest': {'model_path': str(df_model_path), 'skipped': True}
            }
        
        # ML ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
        trainer = MLModelTrainer(self.config)
        
        with profile_execution("ML ëª¨ë¸ í•™ìŠµ"):
            results = trainer.run_training()
            trainer.generate_report(results)
        
        self.benchmark_results['ml_training'] = results
        return results
    
    def run_hybrid_evaluation(self, ml_results: Dict) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€"""
        logger.info("\n" + "="*60)
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ ë‹¨ê³„")
        logger.info("="*60)
        
        # í…ŒìŠ¤íŠ¸í•  ë¯¸ë¡œ ì„ íƒ
        test_ids = self.maze_loader.get_sample_ids('test')
        eval_config = self.config.get('evaluation', {})
        max_test_mazes = eval_config.get('test_mazes', 50)
        
        if len(test_ids) > max_test_mazes:
            test_ids = test_ids[:max_test_mazes]
        
        logger.info(f"í‰ê°€í•  ë¯¸ë¡œ ê°œìˆ˜: {len(test_ids)}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ ì €ì¥
        hybrid_results = {
            'total_mazes': len(test_ids),
            'algorithms': {},
            'comparison': {},
            'individual_results': []
        }
        
        # ACO ì„¤ì •
        aco_config = self.config.get('aco_hybrid', {})
        
        success_counts = {'aco': 0, 'aco_cnn': 0, 'aco_df': 0}
        total_times = {'aco': 0, 'aco_cnn': 0, 'aco_df': 0}
        total_path_lengths = {'aco': [], 'aco_cnn': [], 'aco_df': []}
        
        # ê° ë¯¸ë¡œì— ëŒ€í•´ í‰ê°€
        for i, sample_id in enumerate(test_ids):
            logger.info(f"\në¯¸ë¡œ {i+1}/{len(test_ids)} í‰ê°€ ì¤‘... (ID: {sample_id})")
            
            try:
                # ë¯¸ë¡œ ë°ì´í„° ë¡œë“œ
                img, metadata, array = self.maze_loader.load_sample(sample_id, 'test')
                
                if array is not None:
                    maze_array = array
                else:
                    maze_array = self.maze_loader.convert_image_to_array(img)
                
                # ì‹œì‘ì ê³¼ ëì 
                start_pos = tuple(metadata.get('entrance', (1, 1)))
                goal_pos = tuple(metadata.get('exit', (maze_array.shape[0]-2, maze_array.shape[1]-2)))
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì†”ë²„ ìƒì„±
                solver = HybridSolver(maze_array, start_pos, goal_pos)
                
                # ëª¨ë¸ ë¡œë“œ
                cnn_path = None
                df_path = None
                
                if 'cnn' in ml_results and 'model_paths' in ml_results['cnn']:
                    cnn_path = ml_results['cnn']['model_paths'].get('best')
                elif 'cnn' in ml_results and 'model_path' in ml_results['cnn']:
                    cnn_path = ml_results['cnn']['model_path']
                
                if 'deep_forest' in ml_results and 'model_path' in ml_results['deep_forest']:
                    df_path = ml_results['deep_forest']['model_path']
                
                if cnn_path and os.path.exists(cnn_path):
                    solver.load_cnn_model(cnn_path)
                
                if df_path and os.path.exists(df_path):
                    solver.load_deep_forest_model(df_path)
                
                # ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•´ê²°
                maze_results = solver.solve_all(**aco_config)
                
                # ê²°ê³¼ ì§‘ê³„
                maze_result = {
                    'maze_id': sample_id,
                    'maze_size': maze_array.shape,
                    'start_pos': start_pos,
                    'goal_pos': goal_pos,
                    'algorithms': maze_results
                }
                
                for algo_name, result in maze_results.items():
                    if 'error' not in result:
                        if result['success']:
                            success_counts[algo_name] += 1
                            total_path_lengths[algo_name].append(result['path_length'])
                        
                        total_times[algo_name] += result['execution_time']
                
                hybrid_results['individual_results'].append(maze_result)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                if (i + 1) % 10 == 0:
                    current_success_rates = {
                        name: count / (i + 1) for name, count in success_counts.items()
                    }
                    logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(test_ids)}, ì„±ê³µë¥ : {current_success_rates}")
                
            except Exception as e:
                logger.error(f"ë¯¸ë¡œ {sample_id} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        for algo_name in success_counts.keys():
            avg_time = total_times[algo_name] / len(test_ids)
            success_rate = success_counts[algo_name] / len(test_ids)
            avg_path_length = (sum(total_path_lengths[algo_name]) / len(total_path_lengths[algo_name]) 
                             if total_path_lengths[algo_name] else 0)
            
            hybrid_results['algorithms'][algo_name] = {
                'success_count': success_counts[algo_name],
                'success_rate': success_rate,
                'avg_execution_time': avg_time,
                'avg_path_length': avg_path_length,
                'total_successful_paths': len(total_path_lengths[algo_name])
            }
        
        # ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
        hybrid_results['comparison'] = self._compare_algorithms(hybrid_results['algorithms'])
        
        # ê²°ê³¼ ì €ì¥
        results_file = self.results_dir / 'hybrid_evaluation.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(hybrid_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ ì™„ë£Œ - ê²°ê³¼ ì €ì¥: {results_file}")
        
        return hybrid_results
    
    def _compare_algorithms(self, algorithm_results: Dict) -> Dict:
        """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ"""
        comparison = {
            'best_success_rate': {'algorithm': None, 'value': 0},
            'best_avg_path_length': {'algorithm': None, 'value': float('inf')},
            'fastest_execution': {'algorithm': None, 'value': float('inf')},
            'ranking': []
        }
        
        for algo_name, results in algorithm_results.items():
            # ìµœê³  ì„±ê³µë¥ 
            if results['success_rate'] > comparison['best_success_rate']['value']:
                comparison['best_success_rate'] = {
                    'algorithm': algo_name,
                    'value': results['success_rate']
                }
            
            # ìµœë‹¨ í‰ê·  ê²½ë¡œ
            if (results['avg_path_length'] > 0 and 
                results['avg_path_length'] < comparison['best_avg_path_length']['value']):
                comparison['best_avg_path_length'] = {
                    'algorithm': algo_name,
                    'value': results['avg_path_length']
                }
            
            # ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰
            if results['avg_execution_time'] < comparison['fastest_execution']['value']:
                comparison['fastest_execution'] = {
                    'algorithm': algo_name,
                    'value': results['avg_execution_time']
                }
        
        # ì „ì²´ ë­í‚¹ (ì„±ê³µë¥  ìš°ì„ , ê·¸ ë‹¤ìŒ ê²½ë¡œ ê¸¸ì´)
        ranking_data = []
        for algo_name, results in algorithm_results.items():
            score = results['success_rate'] * 1000 - results['avg_path_length']
            ranking_data.append((algo_name, score, results))
        
        ranking_data.sort(key=lambda x: x[1], reverse=True)
        comparison['ranking'] = [
            {
                'rank': i + 1,
                'algorithm': algo_name,
                'score': score,
                'success_rate': results['success_rate'],
                'avg_path_length': results['avg_path_length']
            }
            for i, (algo_name, score, results) in enumerate(ranking_data)
        ]
        
        return comparison
    
    def generate_final_report(self, ml_results: Dict, hybrid_results: Dict):
        """ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report_path = self.results_dir / 'benchmark_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ë¯¸ë¡œ ì•Œê³ ë¦¬ì¦˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n\n")
            f.write(f"**ìƒì„±ì¼ì‹œ**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**í•˜ë“œì›¨ì–´**: RTX 3060 (6GB VRAM)\n\n")
            
            # ML ëª¨ë¸ í•™ìŠµ ê²°ê³¼
            f.write("## 1. ML ëª¨ë¸ í•™ìŠµ ê²°ê³¼\n\n")
            
            if 'cnn' in ml_results and 'error' not in ml_results['cnn']:
                cnn = ml_results['cnn']
                if not cnn.get('skipped', False):
                    f.write("### CNN ëª¨ë¸\n")
                    f.write(f"- **ìµœê³  ê²€ì¦ ì •í™•ë„**: {cnn.get('best_val_accuracy', 0):.4f}\n")
                    f.write(f"- **í•™ìŠµ ì‹œê°„**: {cnn.get('training_time', 0):.2f}ì´ˆ\n")
                    f.write(f"- **ëª¨ë¸ íŒŒë¼ë¯¸í„°**: {cnn.get('model_info', {}).get('total_parameters', 0):,}\n")
                    f.write(f"- **í”¼í¬ VRAM**: {cnn.get('performance', {}).get('vram_used_mb', {}).get('peak', 0):.1f}MB\n\n")
                else:
                    f.write("### CNN ëª¨ë¸\n- **ìƒíƒœ**: ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš© (í•™ìŠµ ê±´ë„ˆëœ€)\n\n")
            
            if 'deep_forest' in ml_results and 'error' not in ml_results['deep_forest']:
                df = ml_results['deep_forest']
                if not df.get('skipped', False):
                    f.write("### Deep Forest ëª¨ë¸\n")
                    f.write(f"- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {df.get('test_accuracy', 0):.4f}\n")
                    f.write(f"- **í•™ìŠµ ì‹œê°„**: {df.get('training_time', 0):.2f}ì´ˆ\n")
                    f.write(f"- **ìµœì  ë ˆì´ì–´ ìˆ˜**: {df.get('best_layer_count', 0)}\n")
                    f.write(f"- **ì´ íŠ¸ë¦¬ ìˆ˜**: {df.get('model_info', {}).get('total_trees', 'N/A')}\n\n")
                else:
                    f.write("### Deep Forest ëª¨ë¸\n- **ìƒíƒœ**: ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš© (í•™ìŠµ ê±´ë„ˆëœ€)\n\n")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼
            f.write("## 2. í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ\n\n")
            
            f.write(f"**í‰ê°€ ë¯¸ë¡œ ìˆ˜**: {hybrid_results.get('total_mazes', 0)}ê°œ\n\n")
            
            # ì„±ëŠ¥ í…Œì´ë¸”
            f.write("### ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥\n\n")
            f.write("| ì•Œê³ ë¦¬ì¦˜ | ì„±ê³µë¥  | í‰ê·  ê²½ë¡œ ê¸¸ì´ | í‰ê·  ì‹¤í–‰ ì‹œê°„ |\n")
            f.write("|----------|--------|--------------|---------------|\n")
            
            algo_names = {'aco': 'ACO', 'aco_cnn': 'ACO+CNN', 'aco_df': 'ACO+DeepForest'}
            
            for algo_key, algo_display in algo_names.items():
                if algo_key in hybrid_results.get('algorithms', {}):
                    results = hybrid_results['algorithms'][algo_key]
                    f.write(f"| {algo_display} | {results['success_rate']:.3f} | "
                           f"{results['avg_path_length']:.1f} | "
                           f"{results['avg_execution_time']:.3f}ì´ˆ |\n")
            
            f.write("\n")
            
            # ìµœê³  ì„±ëŠ¥
            f.write("### ìµœê³  ì„±ëŠ¥\n\n")
            comparison = hybrid_results.get('comparison', {})
            
            if 'best_success_rate' in comparison:
                best_sr = comparison['best_success_rate']
                algo_name = algo_names.get(best_sr['algorithm'], best_sr['algorithm'])
                f.write(f"- **ìµœê³  ì„±ê³µë¥ **: {algo_name} ({best_sr['value']:.3f})\n")
            
            if 'best_avg_path_length' in comparison:
                best_pl = comparison['best_avg_path_length']
                algo_name = algo_names.get(best_pl['algorithm'], best_pl['algorithm'])
                f.write(f"- **ìµœë‹¨ í‰ê·  ê²½ë¡œ**: {algo_name} ({best_pl['value']:.1f})\n")
            
            if 'fastest_execution' in comparison:
                fastest = comparison['fastest_execution']
                algo_name = algo_names.get(fastest['algorithm'], fastest['algorithm'])
                f.write(f"- **ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰**: {algo_name} ({fastest['value']:.3f}ì´ˆ)\n")
            
            f.write("\n")
            
            # ì „ì²´ ë­í‚¹
            f.write("### ì „ì²´ ë­í‚¹\n\n")
            if 'ranking' in comparison:
                for rank_info in comparison['ranking']:
                    algo_name = algo_names.get(rank_info['algorithm'], rank_info['algorithm'])
                    f.write(f"{rank_info['rank']}. **{algo_name}** - "
                           f"ì„±ê³µë¥ : {rank_info['success_rate']:.3f}, "
                           f"í‰ê·  ê²½ë¡œ: {rank_info['avg_path_length']:.1f}\n")
            
            f.write("\n")
            
            # ê²°ë¡ 
            f.write("## 3. ê²°ë¡ \n\n")
            
            if comparison.get('ranking'):
                winner = comparison['ranking'][0]
                winner_name = algo_names.get(winner['algorithm'], winner['algorithm'])
                f.write(f"**ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜**: {winner_name}\n\n")
                
                if winner['algorithm'] == 'aco':
                    f.write("ê¸°ë³¸ ACOê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ")
                    f.write("ML ëª¨ë¸ì˜ ì¶”ê°€ì ì¸ í•™ìŠµì´ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n")
                else:
                    f.write("í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì´ ê¸°ë³¸ ACOë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ")
                    f.write("ML ëª¨ë¸ì˜ ë„ì›€ìœ¼ë¡œ ë” íš¨ìœ¨ì ì¸ ê²½ë¡œ íƒìƒ‰ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\n\n")
            
            # ì‹œìŠ¤í…œ ì •ë³´
            final_gpu_status = self.profiler.check_rtx3060_limits()
            f.write("## 4. ì‹œìŠ¤í…œ ì •ë³´\n\n")
            f.write(f"- **ìµœì¢… VRAM ì‚¬ìš©ë¥ **: {final_gpu_status['vram_utilization_percent']:.1f}%\n")
            f.write(f"- **í”¼í¬ GPU ì˜¨ë„**: {final_gpu_status['current_metrics']['temperature_c']:.1f}Â°C\n")
            f.write(f"- **ì´ ë²¤ì¹˜ë§ˆí¬ ì‹œê°„**: {time.time() - self.benchmark_start_time:.0f}ì´ˆ\n")
        
        logger.info(f"ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
        
        # ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
        self._print_summary(hybrid_results)
    
    def _print_summary(self, hybrid_results: Dict):
        """ì½˜ì†”ì— ìš”ì•½ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ìš”ì•½")
        print("="*80)
        
        comparison = hybrid_results.get('comparison', {})
        
        if 'ranking' in comparison and comparison['ranking']:
            print("\nğŸ† ì•Œê³ ë¦¬ì¦˜ ìˆœìœ„:")
            for i, rank_info in enumerate(comparison['ranking'][:3]):
                medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
                algo_names = {'aco': 'ACO', 'aco_cnn': 'ACO+CNN', 'aco_df': 'ACO+DeepForest'}
                algo_name = algo_names.get(rank_info['algorithm'], rank_info['algorithm'])
                
                print(f"  {medal} {rank_info['rank']}ìœ„: {algo_name}")
                print(f"      ì„±ê³µë¥ : {rank_info['success_rate']:.1%}")
                print(f"      í‰ê·  ê²½ë¡œ: {rank_info['avg_path_length']:.1f}")
        
        # ì£¼ìš” ì§€í‘œ
        if 'best_success_rate' in comparison:
            best_sr = comparison['best_success_rate']
            print(f"\nâœ… ìµœê³  ì„±ê³µë¥ : {best_sr['value']:.1%} ({best_sr['algorithm'].upper()})")
        
        if 'fastest_execution' in comparison:
            fastest = comparison['fastest_execution']
            print(f"âš¡ ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰: {fastest['value']:.3f}ì´ˆ ({fastest['algorithm'].upper()})")
        
        # GPU ìƒíƒœ
        gpu_status = self.profiler.check_rtx3060_limits()
        print(f"\nğŸ’» ìµœì¢… VRAM ì‚¬ìš©ë¥ : {gpu_status['vram_utilization_percent']:.1f}%")
        
        print("\nğŸ“Š ìƒì„¸ ê²°ê³¼ëŠ” output/results/ ë””ë ‰í„°ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
        print("="*80)
    
    def run_full_benchmark(self) -> Dict:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        self.benchmark_start_time = time.time()
        
        logger.info("ğŸŒ€ ë¯¸ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        logger.info(f"ì„¤ì •: {len(self.config.get('models', []))}ê°œ ML ëª¨ë¸")
        
        # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not self.check_system_requirements():
            logger.error("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            # 1ë‹¨ê³„: ML ëª¨ë¸ í•™ìŠµ
            ml_results = self.train_ml_models()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
            
            # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€
            hybrid_results = self.run_hybrid_evaluation(ml_results)
            
            # 3ë‹¨ê³„: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_final_report(ml_results, hybrid_results)
            
            # ì „ì²´ ê²°ê³¼ ë°˜í™˜
            full_results = {
                'ml_training': ml_results,
                'hybrid_evaluation': hybrid_results,
                'total_time': time.time() - self.benchmark_start_time,
                'config_used': self.config
            }
            
            # ì „ì²´ ê²°ê³¼ ì €ì¥
            full_results_path = self.results_dir / 'full_benchmark_results.json'
            with open(full_results_path, 'w', encoding='utf-8') as f:
                json.dump(full_results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ‰ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {full_results['total_time']:.0f}ì´ˆ")
            
            return full_results
            
        except KeyboardInterrupt:
            logger.info("âŒ ì‚¬ìš©ìì— ì˜í•´ ë²¤ì¹˜ë§ˆí¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return {}
        except Exception as e:
            logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="ë¯¸ë¡œ ì•Œê³ ë¦¬ì¦˜ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
  python scripts/run_benchmark.py

  # íŠ¹ì • ì„¤ì • íŒŒì¼ë¡œ ì‹¤í–‰
  python scripts/run_benchmark.py --config configs/my_config.json

  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì ì€ ë°ì´í„°)
  python scripts/run_benchmark.py --quick-test

  # ML ëª¨ë¸ë§Œ í•™ìŠµ
  python scripts/run_benchmark.py --ml-only

  # í‰ê°€ë§Œ ì‹¤í–‰ (ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©)
  python scripts/run_benchmark.py --eval-only
        """
    )
    
    parser.add_argument(
        '--config', 
        type=str, 
        default='configs/ml_training.json',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: configs/ml_training.json)'
    )
    
    parser.add_argument(
        '--output-dir', 
        type=str, 
        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬ (ì„¤ì • íŒŒì¼ì˜ ê°’ì„ ì˜¤ë²„ë¼ì´ë“œ)'
    )
    
    parser.add_argument(
        '--quick-test', 
        action='store_true',
        help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì ì€ ë°ì´í„°ì™€ ê°„ë‹¨í•œ ì„¤ì •)'
    )
    
    parser.add_argument(
        '--ml-only', 
        action='store_true',
        help='ML ëª¨ë¸ í•™ìŠµë§Œ ì‹¤í–‰'
    )
    
    parser.add_argument(
        '--eval-only', 
        action='store_true',
        help='í‰ê°€ë§Œ ì‹¤í–‰ (ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)'
    )
    
    parser.add_argument(
        '--force-retrain', 
        action='store_true',
        help='ê¸°ì¡´ ëª¨ë¸ì´ ìˆì–´ë„ ê°•ì œë¡œ ì¬í•™ìŠµ'
    )
    
    parser.add_argument(
        '--gpu-check', 
        action='store_true',
        help='GPU ìƒíƒœë§Œ í™•ì¸í•˜ê³  ì¢…ë£Œ'
    )
    
    parser.add_argument(
        '--models', 
        nargs='+', 
        choices=['cnn', 'deep_forest'],
        help='í•™ìŠµí•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: ëª¨ë“  ëª¨ë¸)'
    )
    
    parser.add_argument(
        '--test-mazes', 
        type=int,
        help='í‰ê°€ì— ì‚¬ìš©í•  ë¯¸ë¡œ ê°œìˆ˜'
    )
    
    parser.add_argument(
        '--verbose', '-v', 
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    return parser.parse_args()


def modify_config_for_quick_test(config: Dict) -> Dict:
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„¤ì • ìˆ˜ì •"""
    config = config.copy()
    
    # ë°ì´í„° í¬ê¸° ì¶•ì†Œ
    config['data_limits'] = {
        'max_train_samples': 100,
        'max_valid_samples': 30,
        'max_test_samples': 10
    }
    
    # CNN ì„¤ì • ê°„ì†Œí™”
    config['cnn'].update({
        'epochs': 2,
        'batch_size': 4
    })
    
    # Deep Forest ì„¤ì • ê°„ì†Œí™”
    config['deep_forest'].update({
        'n_layers': 1,
        'n_estimators': 20,
        'max_depth': 5
    })
    
    # ACO ì„¤ì • ê°„ì†Œí™”
    config['aco_hybrid'].update({
        'n_ants': 10,
        'n_iterations': 20,
        'max_steps': 200
    })
    
    # í‰ê°€ ë¯¸ë¡œ ìˆ˜ ì¶•ì†Œ
    config['evaluation']['test_mazes'] = 5
    
    return config


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # GPU ìƒíƒœë§Œ í™•ì¸
    if args.gpu_check:
        profiler = get_profiler()
        gpu_status = profiler.check_rtx3060_limits()
        
        print("ğŸ”§ GPU ìƒíƒœ í™•ì¸")
        print("="*50)
        print(f"VRAM ì‚¬ìš©ë¥ : {gpu_status['vram_utilization_percent']:.1f}%")
        print(f"í˜„ì¬ VRAM: {gpu_status['current_metrics']['vram_used_mb']:.1f}MB")
        print(f"ì´ VRAM: {gpu_status['current_metrics']['vram_total_mb']:.1f}MB")
        print(f"GPU ì˜¨ë„: {gpu_status['current_metrics']['temperature_c']:.1f}Â°C")
        print(f"ì „ë ¥ ì†Œë¹„: {gpu_status['current_metrics']['power_watts']:.1f}W")
        
        if gpu_status['warnings']:
            print("\nâš ï¸ ê²½ê³  ì‚¬í•­:")
            for warning in gpu_status['warnings']:
                print(f"  - {warning}")
        else:
            print("\nâœ… ëª¨ë“  ì§€í‘œê°€ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤.")
        
        return
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    if not os.path.exists(args.config):
        logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        logger.info("ê¸°ë³¸ ì„¤ì • íŒŒì¼ì„ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (configs/ml_training.json)")
        
        # ê¸°ë³¸ ì„¤ì • ìƒì„±
        os.makedirs('configs', exist_ok=True)
        
        # ì„¤ì • í…œí”Œë¦¿ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ê±°ë‚˜ ë³„ë„ íŒŒì¼ì—ì„œ ë³µì‚¬
        logger.error("ë¨¼ì € ì„¤ì • íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        sys.exit(1)
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        benchmark = MazeBenchmark(args.config)
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        if args.output_dir:
            benchmark.config['output_dir'] = args.output_dir
        
        if args.models:
            benchmark.config['models'] = args.models
        
        if args.test_mazes:
            benchmark.config['evaluation']['test_mazes'] = args.test_mazes
        
        if args.force_retrain:
            benchmark.config['force_retrain'] = True
        
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        if args.quick_test:
            logger.info("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")
            benchmark.config = modify_config_for_quick_test(benchmark.config)
        
        # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if args.ml_only:
            logger.info("ğŸ¤– ML ëª¨ë¸ í•™ìŠµë§Œ ì‹¤í–‰")
            results = benchmark.train_ml_models()
            print(f"\nâœ… ML ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            
        elif args.eval_only:
            logger.info("ğŸ¯ í‰ê°€ë§Œ ì‹¤í–‰ (ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©)")
            
            # ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            ml_results = {
                'cnn': {'model_path': str(benchmark.models_dir / 'best_cnn_model.pth')},
                'deep_forest': {'model_path': str(benchmark.models_dir / 'deep_forest_model.joblib')}
            }
            
            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            missing_models = []
            for model_name, result in ml_results.items():
                if not os.path.exists(result['model_path']):
                    missing_models.append(model_name)
            
            if missing_models:
                logger.error(f"ë‹¤ìŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {missing_models}")
                logger.info("ë¨¼ì € --ml-onlyë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
                sys.exit(1)
            
            hybrid_results = benchmark.run_hybrid_evaluation(ml_results)
            benchmark.generate_final_report(ml_results, hybrid_results)
            print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
            
        else:
            # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            logger.info("ğŸŒ€ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
            results = benchmark.run_full_benchmark()
            
            if results:
                print(f"\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
                print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í„°ë¦¬: {benchmark.results_dir}")
            else:
                print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ ë˜ëŠ” ì¤‘ë‹¨ë¨")
                sys.exit(1)
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()